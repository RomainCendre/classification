{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sheet properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/rcendre/classification')\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\";\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\";  \n",
    "from numpy import array, ones, maximum\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from toolbox.models.builtin import Applications\n",
    "from toolbox.classification.common import IO, Tools, Folds\n",
    "from toolbox.classification.parameters import Settings, Dermatology\n",
    "from toolbox.transforms.labels import OrderedEncoder\n",
    "from toolbox.views.common import Views, ViewsTools\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = 4\n",
    "settings = Settings.get_default_dermatology()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Dermatology.images(modality='Microscopy', data_type='Full', use_unknown=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform groups\n",
    "group_encoder = LabelEncoder().fit(array(inputs['ID_Patient'].tolist()))\n",
    "Tools.transform(inputs, {'datum': 'ID_Patient'}, group_encoder, 'GroupEncode')\n",
    "# Transform labels\n",
    "label_encoder = OrderedEncoder().fit(['Normal', 'Benign', 'Malignant'])\n",
    "Tools.transform(inputs, {'datum': 'Label'}, label_encoder, 'LabelEncode')\n",
    "# Make folds\n",
    "Folds.build_group_folds(inputs, {'datum': 'Datum', 'label_encode': 'LabelEncode', 'group': 'GroupEncode'}, validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additionnal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "additionnal = {'batch_size': 8,\n",
    "            'epochs' : 15,\n",
    "            'shuffle': True,\n",
    "            'rotation_range': 45,\n",
    "            'width_shift_range': 0.1,\n",
    "            'height_shift_range': 0.1,\n",
    "            'horizontal_flip': True,\n",
    "            'vertical_flip': True,\n",
    "            'fill_mode': 'wrap'}\n",
    "model = Applications.get_fine_tuning(3, 0, -1, architecture='ResNet', pooling='max', activation='softmax', additional=additionnal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5431 images belonging to 3 classes.\n",
      "Pre-training...\n",
      "WARNING:tensorflow:From /home/rcendre/classification/toolbox/models/models.py:807: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 679 steps\n",
      "Epoch 1/15\n",
      "679/679 [==============================] - 1146s 2s/step - loss: 6.1036 - accuracy: 0.5404\n",
      "Epoch 2/15\n",
      "679/679 [==============================] - 1146s 2s/step - loss: 5.4405 - accuracy: 0.6080\n",
      "Epoch 3/15\n",
      "679/679 [==============================] - 1150s 2s/step - loss: 4.7026 - accuracy: 0.6369\n",
      "Epoch 4/15\n",
      "679/679 [==============================] - 1157s 2s/step - loss: 4.1553 - accuracy: 0.6586\n",
      "Epoch 5/15\n",
      "679/679 [==============================] - 1145s 2s/step - loss: 4.2468 - accuracy: 0.6684\n",
      "Epoch 6/15\n",
      "679/679 [==============================] - 1142s 2s/step - loss: 3.8542 - accuracy: 0.6844\n",
      "Epoch 7/15\n",
      "679/679 [==============================] - 1145s 2s/step - loss: 4.4743 - accuracy: 0.6702\n",
      "Epoch 8/15\n",
      "679/679 [==============================] - 1144s 2s/step - loss: 4.1005 - accuracy: 0.6927\n",
      "Epoch 9/15\n",
      "679/679 [==============================] - 1145s 2s/step - loss: 4.1057 - accuracy: 0.6846\n",
      "Epoch 10/15\n",
      "679/679 [==============================] - 1143s 2s/step - loss: 4.8211 - accuracy: 0.6763\n",
      "Epoch 11/15\n",
      "679/679 [==============================] - 1147s 2s/step - loss: 4.5143 - accuracy: 0.6949\n",
      "Epoch 12/15\n",
      "679/679 [==============================] - 1145s 2s/step - loss: 3.9520 - accuracy: 0.7001\n",
      "Epoch 13/15\n",
      "679/679 [==============================] - 1147s 2s/step - loss: 4.3727 - accuracy: 0.6953\n",
      "Epoch 14/15\n",
      "679/679 [==============================] - 1145s 2s/step - loss: 3.8068 - accuracy: 0.7199\n",
      "Epoch 15/15\n",
      "679/679 [==============================] - 1147s 2s/step - loss: 3.7044 - accuracy: 0.7124\n",
      "Final-training...\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 679 steps\n",
      "Epoch 1/15\n",
      "679/679 [==============================] - 1171s 2s/step - loss: 5.5037 - accuracy: 0.6932\n",
      "Epoch 2/15\n",
      "679/679 [==============================] - 1164s 2s/step - loss: 1.4595 - accuracy: 0.7892\n",
      "Epoch 3/15\n",
      "679/679 [==============================] - 1167s 2s/step - loss: 0.8742 - accuracy: 0.8148\n",
      "Epoch 4/15\n",
      "679/679 [==============================] - 1171s 2s/step - loss: 0.5854 - accuracy: 0.8556\n",
      "Epoch 5/15\n",
      "679/679 [==============================] - 1168s 2s/step - loss: 0.5702 - accuracy: 0.8532\n",
      "Epoch 6/15\n",
      "679/679 [==============================] - 1171s 2s/step - loss: 0.4088 - accuracy: 0.8781\n",
      "Epoch 7/15\n",
      "679/679 [==============================] - 1175s 2s/step - loss: 0.3988 - accuracy: 0.8785\n",
      "Epoch 8/15\n",
      "679/679 [==============================] - 1171s 2s/step - loss: 0.3131 - accuracy: 0.8967\n",
      "Epoch 9/15\n",
      "679/679 [==============================] - 1171s 2s/step - loss: 0.3487 - accuracy: 0.8949\n",
      "Epoch 10/15\n",
      "679/679 [==============================] - 1171s 2s/step - loss: 0.2929 - accuracy: 0.9050\n",
      "Epoch 11/15\n",
      "679/679 [==============================] - 1173s 2s/step - loss: 0.2862 - accuracy: 0.9044\n",
      "Epoch 12/15\n",
      "679/679 [==============================] - 1174s 2s/step - loss: 0.2518 - accuracy: 0.9131\n",
      "Epoch 13/15\n",
      "679/679 [==============================] - 1170s 2s/step - loss: 0.2884 - accuracy: 0.9057\n",
      "Epoch 14/15\n",
      "679/679 [==============================] - 1171s 2s/step - loss: 0.2342 - accuracy: 0.9217\n",
      "Epoch 15/15\n",
      "679/679 [==============================] - 1168s 2s/step - loss: 0.2015 - accuracy: 0.9330\n",
      "WARNING:tensorflow:From /home/rcendre/anaconda3/envs/PythonGPU/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: FineVisualisation/assets\n"
     ]
    }
   ],
   "source": [
    "model = Tools.fit(inputs, {'datum': 'Datum', 'label_encode': 'LabelEncode'}, model)\n",
    "model.save('FineVisualisation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "model.load('FineVisualisation')\n",
    "\n",
    "from toolbox.views.images import GradCAM\n",
    "\n",
    "for IMAGE_INDEX in range(0,1000):\n",
    "    image_class = inputs['LabelEncode'][IMAGE_INDEX]\n",
    "    image_path = inputs['Datum'][IMAGE_INDEX]\n",
    "    image = tf.keras.preprocessing.image.load_img(image_path)\n",
    "    image = tf.keras.preprocessing.image.img_to_array(image)\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    image_size = image.shape\n",
    "\n",
    "    # initialize our gradient class activation map and build the heatmap\n",
    "    cam = GradCAM(model.model, image_class)\n",
    "    heatmap = cam.compute_heatmap(image)\n",
    "    cam = cv2.applyColorMap(np.uint8(255*heatmap), cv2.COLORMAP_JET)\n",
    "\n",
    "    output_image = cv2.addWeighted(cv2.cvtColor(image[0].astype('uint8'), cv2.COLOR_RGB2BGR), 1, cam, 0.25, 0)\n",
    "\n",
    "    cv2.imwrite(f'FineVis/{IMAGE_INDEX}_{image_class}.png', output_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
